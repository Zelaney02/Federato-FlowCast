{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7HO4prnTKC_H"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install ijson\n",
        "!pip install numpy pandas matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8niGJOaDgm0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Rp9BhEmoKWrH"
      },
      "outputs": [],
      "source": [
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9nGOW4neKl7v"
      },
      "outputs": [],
      "source": [
        "# Replace with your shared drive ID\n",
        "shared_drive_id = '1WcjMv_jvTMpqeI5N4IKa0g6TbeR-h3aY'\n",
        "\n",
        "# List files in the shared drive\n",
        "file_list = drive.ListFile({'q': f\"'{shared_drive_id}' in parents and trashed=false\"}).GetList()\n",
        "\n",
        "for file in file_list:\n",
        "    print(f\"Title: {file['title']}, ID: {file['id']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9jRTxOw0Imj0"
      },
      "outputs": [],
      "source": [
        "# Example: Download a file from the shared drive\n",
        "file_id = '1uSoQRTZq6KWbFS6aX63w3WNQaiQQf0B_'  # Replace with the file ID\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile(downloaded['title'])  # Download the file\n",
        "print(f\"Downloaded: {downloaded['title']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6puubIlYK3KT"
      },
      "outputs": [],
      "source": [
        "# Example: Download a file from the shared drive\n",
        "file_id = '1oxCDgVNoHgWWYHC9z5eicliLr8O7-Odh'  # Replace with the file ID\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile(downloaded['title'])  # Download the file\n",
        "print(f\"Downloaded: {downloaded['title']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ChkNv2ehLZKl"
      },
      "outputs": [],
      "source": [
        "!unzip /content/amplitude_export_2024.zip -d /content/2024_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_sGgD-6Lo7l",
        "outputId": "8c437ad8-f4e4-4dbb-9d46-cdf3505ce8d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/amplitude_export_2025.json.zip\n",
            "  inflating: /content/2025_json/new_amplitude_export_2025.json  "
          ]
        }
      ],
      "source": [
        "!unzip /content/amplitude_export_2025.json.zip -d /content/2025_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VSPw8Lc1O0BM"
      },
      "outputs": [],
      "source": [
        "# import ijson\n",
        "# import pandas as pd\n",
        "# from pathlib import Path\n",
        "\n",
        "# #file year\n",
        "# year = 2025\n",
        "# file_path = f\"2025_json/new_amplitude_export_2025.json\"\n",
        "\n",
        "# #non-empty columns\n",
        "# columns_keep = [\n",
        "#     \"$insert_id\",\n",
        "#     \"amplitude_id\",\n",
        "#     \"app\",\n",
        "#     \"city\",\n",
        "#     \"client_event_time\",\n",
        "#     \"client_upload_time\",\n",
        "#     \"country\",\n",
        "#     \"data\",\n",
        "#     \"data_type\",\n",
        "#     \"device_family\",\n",
        "#     \"device_id\",\n",
        "#     \"device_type\",\n",
        "#     \"dma\",\n",
        "#     \"event_id\",\n",
        "#     \"event_properties\",\n",
        "#     \"event_time\",\n",
        "#     \"event_type\",\n",
        "#     \"language\",\n",
        "#     \"library\",\n",
        "#     \"os_name\",\n",
        "#     \"os_version\",\n",
        "#     \"platform\",\n",
        "#     \"processed_time\",\n",
        "#     \"region\",\n",
        "#     \"server_received_time\",\n",
        "#     \"server_upload_time\",\n",
        "#     \"session_id\",\n",
        "#     \"user_id\",\n",
        "#     \"user_properties\",\n",
        "#     \"uuid\",\n",
        "# ]\n",
        "# path = Path(f\"{year}_csv\")\n",
        "# if not path.exists():\n",
        "#     path.mkdir(parents=True, exist_ok=True)\n",
        "# #use ijson to read the json files efficiently in memory\n",
        "# with open(file_path, \"r\") as f:\n",
        "#     objects = ijson.items(f, \"item\") #creates a generator object\n",
        "\n",
        "#     batch_size = 100000 #can be updated, currently saves per batches of 100,000\n",
        "#     chunk = []\n",
        "#     count = 0 #used to index batch file\n",
        "#     for obj in objects:\n",
        "#         chunk.append(obj)\n",
        "#         if len(chunk) >= batch_size:\n",
        "#             df = pd.DataFrame(chunk)\n",
        "#             output_csv = f\"{year}_csv/amplitude_export_chunk_{count*batch_size}_{(count+1)*batch_size}.csv\"\n",
        "#             df = df[columns_keep] #remove empty columns\n",
        "#             df.to_csv(output_csv, index=False)\n",
        "#             count += 1\n",
        "#             chunk = []\n",
        "\n",
        "#     if chunk: #process remaining data if any\n",
        "#         output_csv = f\"{year}_csv/amplitude_export_chunk_{count*batch_size}_{(count+1)*batch_size}.csv\"\n",
        "#         df = pd.DataFrame(chunk)\n",
        "#         df = df[columns_keep]\n",
        "#         df.to_csv(output_csv, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jotDvtIAYgFs"
      },
      "outputs": [],
      "source": [
        "# import ijson\n",
        "# import os\n",
        "# import pandas as pd\n",
        "# from pathlib import Path\n",
        "\n",
        "# #file year\n",
        "# year = 2024\n",
        "# folder_path = f\"2024_json/new_export\"\n",
        "\n",
        "# #non-empty columns\n",
        "# columns_keep = [\n",
        "#     \"$insert_id\",\n",
        "#     \"amplitude_id\",\n",
        "#     \"app\",\n",
        "#     \"city\",\n",
        "#     \"client_event_time\",\n",
        "#     \"client_upload_time\",\n",
        "#     \"country\",\n",
        "#     \"data\",\n",
        "#     \"data_type\",\n",
        "#     \"device_family\",\n",
        "#     \"device_id\",\n",
        "#     \"device_type\",\n",
        "#     \"dma\",\n",
        "#     \"event_id\",\n",
        "#     \"event_properties\",\n",
        "#     \"event_time\",\n",
        "#     \"event_type\",\n",
        "#     \"language\",\n",
        "#     \"library\",\n",
        "#     \"os_name\",\n",
        "#     \"os_version\",\n",
        "#     \"platform\",\n",
        "#     \"processed_time\",\n",
        "#     \"region\",\n",
        "#     \"server_received_time\",\n",
        "#     \"server_upload_time\",\n",
        "#     \"session_id\",\n",
        "#     \"user_id\",\n",
        "#     \"user_properties\",\n",
        "#     \"uuid\",\n",
        "# ]\n",
        "# path = Path(f\"{year}_csv\")\n",
        "# if not path.exists():\n",
        "#     path.mkdir(parents=True, exist_ok=True)\n",
        "# #use ijson to read the json files efficiently in memory\n",
        "# for file_name in os.listdir(folder_path):\n",
        "#   file_path = os.path.join(folder_path, file_name)\n",
        "#   with open(file_path, \"r\") as f:\n",
        "#       objects = ijson.items(f, \"item\") #creates a generator object\n",
        "\n",
        "#       batch_size = 100000 #can be updated, currently saves per batches of 100,000\n",
        "#       chunk = []\n",
        "#       count = 0 #used to index batch file\n",
        "#       for obj in objects:\n",
        "#           chunk.append(obj)\n",
        "#           if len(chunk) >= batch_size:\n",
        "#               df = pd.DataFrame(chunk)\n",
        "#               output_csv = f\"{year}_csv/{file_name.split('.')[0]}_chunk_{count*batch_size}_{(count+1)*batch_size}.csv\"\n",
        "#               df = df[columns_keep] #remove empty columns\n",
        "#               df.to_csv(output_csv, index=False)\n",
        "#               count += 1\n",
        "#               chunk = []\n",
        "\n",
        "#       if chunk: #process remaining data if any\n",
        "#           output_csv = f\"{year}_csv/{file_name.split('.')[0]}_chunk_{count*batch_size}_{(count+1)*batch_size}.csv\"\n",
        "#           df = pd.DataFrame(chunk)\n",
        "#           df = df[columns_keep]\n",
        "#           df.to_csv(output_csv, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bTeKLyBflhSN",
        "outputId": "7056d703-1166-482e-f31a-a4e69d3d7738"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Expected object or value",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9c1801a07fa8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Read JSON file in chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Filter columns and clean data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns_keep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype_backend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m             \u001b[0mlines_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0;31m# Make sure that the returned objects have the right index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1403\u001b[0;31m                 \u001b[0mujson_loads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m             )\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected object or value"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# File year\n",
        "year = 2024\n",
        "folder_path = f\"2024_json/new_export\"\n",
        "\n",
        "# Non-empty columns\n",
        "columns_keep = [\n",
        "    \"client_event_time\",\n",
        "    \"client_upload_time\",\n",
        "    \"event_time\",\n",
        "    \"event_type\",\n",
        "    \"processed_time\",\n",
        "    \"server_received_time\",\n",
        "    \"server_upload_time\",\n",
        "    \"session_id\",\n",
        "    \"user_id\",\n",
        "]\n",
        "\n",
        "# Create output directory\n",
        "path = Path(f\"{year}_csv\")\n",
        "if not path.exists():\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Process files in chunks\n",
        "for file_name in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    chunk_size = 100000  # Adjust based on your memory constraints\n",
        "    chunk_count = 0\n",
        "\n",
        "    # Read JSON file in chunks\n",
        "    for chunk in pd.read_json(file_path, lines=True, chunksize=chunk_size):\n",
        "        # Filter columns and clean data\n",
        "        chunk = chunk[columns_keep]\n",
        "        chunk = chunk.replace(\"EMPTY\", pd.NA).dropna()\n",
        "\n",
        "        # Save chunk to CSV\n",
        "        output_csv = f\"{year}_csv/{file_name.split('.')[0]}_chunk_{chunk_count}.csv\"\n",
        "        chunk.to_csv(output_csv, index=False)\n",
        "        chunk_count += 1\n",
        "\n",
        "        # Optionally, append to a master DataFrame (if needed)\n",
        "        # mass2024df = pd.concat([mass2024df, chunk], ignore_index=True)\n",
        "\n",
        "# If you need to combine all chunks into a single DataFrame, load them back in chunks\n",
        "mass2024df = pd.concat(\n",
        "    (pd.read_csv(f) for f in path.glob(\"*.csv\")),\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "# Analyze event counts\n",
        "event_counts2024 = mass2024df['event_type'].value_counts()\n",
        "threshold24 = 0.001 * len(mass2024df)\n",
        "common_event_types2024 = event_counts2024[event_counts2024 > threshold24].index.tolist()\n",
        "filtered_events2024 = event_counts2024[event_counts2024 > threshold24]\n",
        "\n",
        "# Plot\n",
        "filtered_events2024.plot(kind='bar', title='Class Distribution (Events > 0.1% of the time)')\n",
        "plt.xlabel(\"Event Type\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9PIh3cRglhpR"
      },
      "outputs": [],
      "source": [
        "import ijson\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "#file year\n",
        "year = 2025\n",
        "file_path = f\"2025_json/new_amplitude_export_2025.json\"\n",
        "\n",
        "#non-empty columns\n",
        "columns_keep = [\n",
        "    \"client_event_time\",\n",
        "    \"client_upload_time\",\n",
        "    \"event_time\",\n",
        "    \"event_type\",\n",
        "    \"processed_time\",\n",
        "    \"server_received_time\",\n",
        "    \"server_upload_time\",\n",
        "    \"session_id\",\n",
        "    \"user_id\",\n",
        "]\n",
        "mass2025df = pd.DataFrame()\n",
        "path = Path(f\"{year}_csv\")\n",
        "if not path.exists():\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "#use ijson to read the json files efficiently in memory\n",
        "with open(file_path, \"r\") as f:\n",
        "    objects = ijson.items(f, \"item\") #creates a generator object\n",
        "\n",
        "    batch_size = 100000 #can be updated, currently saves per batches of 100,000\n",
        "    chunk = []\n",
        "    count = 0 #used to index batch file\n",
        "    for obj in objects:\n",
        "        chunk.append(obj)\n",
        "        if len(chunk) >= batch_size:\n",
        "            df = pd.DataFrame(chunk)\n",
        "            output_csv = f\"{year}_csv/amplitude_export_chunk_{count*batch_size}_{(count+1)*batch_size}.csv\"\n",
        "            df = df[columns_keep] #remove empty columns\n",
        "            df = df.replace(\"EMPTY\", pd.NA).dropna()\n",
        "            mass2025df = pd.concat([mass2025df, df], ignore_index=True)\n",
        "            df.to_csv(output_csv, index=False)\n",
        "            count += 1\n",
        "            chunk = []\n",
        "\n",
        "    if chunk: #process remaining data if any\n",
        "        output_csv = f\"{year}_csv/amplitude_export_chunk_{count*batch_size}_{(count+1)*batch_size}.csv\"\n",
        "        df = pd.DataFrame(chunk)\n",
        "        df = df[columns_keep]\n",
        "        df = df.replace(\"None\", pd.NA).dropna()\n",
        "        mass2025df = pd.concat([mass2025df, df], ignore_index=True)\n",
        "        df.to_csv(output_csv, index=False)\n",
        "event_counts = mass2025df['event_type'].value_counts()\n",
        "threshold = 0.001 * len(mass2025df)\n",
        "common_event_types = event_counts[event_counts > threshold].index.tolist()\n",
        "filtered_events = event_counts[event_counts > threshold]\n",
        "filtered_events.plot(kind='bar', title='Class Distribution (Events > 0.1% of the time)')\n",
        "plt.xlabel(\"Event Type\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h395tJ-2HLzs"
      },
      "source": [
        "This model is for the 2025 preprocessed dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-T1JfZ17Uaex"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Assuming df is your cleaned DataFrame\n",
        "# Sort the data by session_id and event_time\n",
        "mass2025df = mass2025df.sort_values(by=['session_id', 'event_time'])\n",
        "\n",
        "# Create the event_before column\n",
        "mass2025df['event_before'] = mass2025df.groupby('session_id')['event_type'].shift()\n",
        "\n",
        "# Drop the first row of each session since it won't have an event_before\n",
        "mass2025df = mass2025df.dropna(subset=['event_before'])\n",
        "\n",
        "# Prepare the data for Markov chain training\n",
        "transition_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for _, row in mass2025df.iterrows():\n",
        "    transition_counts[row['event_before']][row['event_type']] += 1\n",
        "\n",
        "# Convert counts to probabilities\n",
        "transition_probabilities = defaultdict(dict)\n",
        "\n",
        "for event_before, transitions in transition_counts.items():\n",
        "    total = sum(transitions.values())\n",
        "    for event_type, count in transitions.items():\n",
        "        transition_probabilities[event_before][event_type] = count / total\n",
        "\n",
        "# Now you have a transition probability matrix\n",
        "# You can use this to predict the next event given the current event\n",
        "\n",
        "# Example: Predict the next event given the current event\n",
        "def predict_next_event(current_event):\n",
        "    if current_event in transition_probabilities:\n",
        "        next_event_probs = transition_probabilities[current_event]\n",
        "        return max(next_event_probs, key=next_event_probs.get)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "event_predictions = {\"current_event\": [], \"next_event\": []}\n",
        "\n",
        "for current_event in common_event_types:\n",
        "    next_event = predict_next_event(current_event)\n",
        "    event_predictions[\"current_event\"].append(current_event)\n",
        "    event_predictions[\"next_event\"].append(next_event)\n",
        "\n",
        "event_df = pd.DataFrame(event_predictions)\n",
        "\n",
        "display(event_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}